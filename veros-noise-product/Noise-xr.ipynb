{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import mlab\n",
    "%matplotlib widget\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from statsmodels.regression.linear_model import yule_walker\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from eofs.standard import Eof\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import xesmf as xe\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from general_purpose import cartopy_plots as cplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_product_name='atlantic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The detrended temperature anomaly data is imported and resampled on the grid used in the VEROS simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Veros grid and landmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = xr.open_dataarray('landmask_veros4x4.nc')\n",
    "vlon = lm.lon.data\n",
    "vlat = lm.lat.data\n",
    "LON, LAT = np.meshgrid(vlon, vlat)\n",
    "landMask = np.where(lm.data == 1, np.nan, 1)\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(1)\n",
    "fig = plt.figure(num=1, figsize=(8,4))\n",
    "ax = fig.add_subplot(111, projection=cplt.ccrs.PlateCarree())\n",
    "\n",
    "cplt.geo_plotter(ax, LON, LAT,\n",
    "                 landMask,\n",
    "                 greenwich=True,\n",
    "                 draw_gridlines=False, draw_labels=False,\n",
    "                 put_colorbar=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the detrended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrend_data = xr.open_dataarray('HEAVY--detrended_temperature_1870-_no_seasonal.nc')\n",
    "detrend_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regridder = xe.Regridder(detrend_data, lm, 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regridded_data = regridder(detrend_data)\n",
    "regridded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartopy.util import add_cyclic_point as acp\n",
    "# old regridding: does not take into account the spherical shape of the Earth\n",
    "f = RegularGridInterpolator((detrend_data.time.data,\n",
    "                             detrend_data.latitude.data,\n",
    "                             list(detrend_data.longitude.data) + [detrend_data.longitude.data[0] + 360]),\n",
    "                            acp(detrend_data.data, axis=2))\n",
    "\n",
    "tt,yt,xt = np.meshgrid(detrend_data.time.data,\n",
    "                       vlat,\n",
    "                       np.array([(v + 180) % 360 - 180 if v != -180 else 180 for v in vlon]),\n",
    "                       indexing = 'ij') \n",
    "test_points = np.array([tt.ravel(),yt.ravel(),xt.ravel()]).T\n",
    "\n",
    "rescaledGrids = f(test_points).reshape(detrend_data.time.shape[0], 40,90)\n",
    "regridded_data = xr.DataArray(rescaledGrids, coords=regridded_data.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that regridding worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_id = 240\n",
    "\n",
    "plt.close(2)\n",
    "fig = plt.figure(num=2, figsize=(8,8))\n",
    "ax = fig.add_subplot(211, projection=cplt.ccrs.PlateCarree())\n",
    "\n",
    "levels = np.linspace(-5,5,11)\n",
    "\n",
    "cplt.geo_plotter(ax, *np.meshgrid(detrend_data.longitude,detrend_data.latitude),\n",
    "                 detrend_data.isel(time=month_id).data,\n",
    "                 levels=levels,\n",
    "                 greenwich=True,\n",
    "                 draw_gridlines=False, draw_labels=False,\n",
    "                 put_colorbar=True,\n",
    "                 title='original'\n",
    "                )\n",
    "ax.set_extent([-180,180,-90,90], crs=cplt.ccrs.PlateCarree())\n",
    "\n",
    "ax = fig.add_subplot(212, projection=cplt.ccrs.PlateCarree())\n",
    "\n",
    "cplt.geo_plotter(ax, *np.meshgrid(regridded_data.lon,regridded_data.lat),\n",
    "                 regridded_data.isel(time=month_id).data,\n",
    "                 greenwich=True,\n",
    "                 levels=levels,\n",
    "                 draw_gridlines=False, draw_labels=False,\n",
    "                 put_colorbar=True,\n",
    "                 title='regridded'\n",
    "                )\n",
    "ax.set_extent([-180,180,-90,90], crs=cplt.ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask only the Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_mask = np.copy(landMask)\n",
    "\n",
    "# mask out everything below 35 deg south\n",
    "atlantic_mask[fvlat < -35, :] = np.nan\n",
    "\n",
    "atlantic_mask[:, vlon > 25] = np.nan\n",
    "atlantic_mask[:, vlon < -120] = np.nan\n",
    "\n",
    "for i in range(atlantic_mask.shape[0]):\n",
    "    if fvlat[i] > 20 or fvlat[i] < -35:\n",
    "        continue\n",
    "    j = np.nanargmax(atlantic_mask[i])\n",
    "    while not np.isnan(atlantic_mask[i,j]):\n",
    "        j += 1\n",
    "    atlantic_mask[i,:j] = np.nan\n",
    "    \n",
    "atlantic_mask[:, vlon < -100] = np.nan\n",
    "\n",
    "plt.close(2)\n",
    "fig = plt.figure(num=2, figsize=(4,4))\n",
    "ax = fig.add_subplot(111, projection=cplt.ccrs.PlateCarree())\n",
    "\n",
    "cplt.geo_plotter(ax, LON, LAT,\n",
    "                 atlantic_mask,\n",
    "                 greenwich=True,\n",
    "                 draw_gridlines=False, draw_labels=False,\n",
    "                 put_colorbar=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landMask = atlantic_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOFs and PCs\n",
    "Now that the data has been resampled, we can do the EOF analysis. Here we use the EOFs expressed as covariance and use the normalized PCs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_re = Eof(regridded_data.fillna(0).data*landMask) # The EOFs are calculated after applying the landmask, since we are only interested in the ocean dynamics\n",
    "eof_re = solver_re.eofsAsCovariance()\n",
    "pc_re = solver_re.pcs(pcscaling = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check we reconstruct a grid from the data using the EOFs and PCs and plot the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_id = 240\n",
    "grid0 = np.tensordot(pc_re[month_id],eof_re*landMask, axes = ([0],[0])) #linear combination of EOFs using PC coefficients\n",
    "\n",
    "cplt.mfp(LON, LAT, np.stack([grid0, regridded_data.data[month_id]*landMask, regridded_data.data[month_id]*landMask-grid0], axis=-1),\n",
    "         extents=None,\n",
    "         greenwich=True,\n",
    "         # mode='contourf',\n",
    "         projections=cplt.ccrs.PlateCarree(),\n",
    "         titles=['reconstructed', 'original', 'difference'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_id = 1000\n",
    "grid0 = np.tensordot(pc_re[month_id],eof_re*landMask, axes = ([0],[0])) #linear combination of EOFs using PC coefficients\n",
    "plt.figure()\n",
    "plt.imshow(grid0)# PCs and EOFs reproced grid\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.imshow(regridded_data.data[month_id]*landMask) # True data grid\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.imshow(regridded_data.data[month_id]*landMask-grid0)#Difference\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the fraction of the variance explained by the number of EOFs used and determine how many EOFs to use in the AR-model, based on some cut-off. A larger cut off leads to more EOFs which is more computationally intesive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varfrac = 0.9\n",
    "\n",
    "\n",
    "var_pc_re = solver_re.varianceFraction()\n",
    "var_pc_re_sum = np.cumsum(var_pc_re)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(var_pc_re_sum)\n",
    "plt.xlabel('Nr. of EOFs')\n",
    "plt.ylabel('Variace Fraction')\n",
    "plt.grid(alpha = 0.95)\n",
    "plt.axhline(varfrac, label=f'{varfrac*100}%', color='red')\n",
    "plt.legend()\n",
    "\n",
    "def pick_pc_re(exp_var):\n",
    "    print(np.argmin(np.abs(var_pc_re_sum-exp_var)))\n",
    "    return np.argmin(np.abs(var_pc_re_sum-exp_var))\n",
    "\n",
    "nr_eofs = int(pick_pc_re(varfrac)) # Print the number of EOFs needed to have a var. frac. of 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmax = np.nanmax(np.abs(eof_re))\n",
    "ofl = (3,8)\n",
    "start = 12\n",
    "\n",
    "plt.close('all')\n",
    "plots_per_fig = np.prod(ofl)\n",
    "i = 0\n",
    "while True:\n",
    "    lower = start + plots_per_fig*i\n",
    "    upper = min(start + plots_per_fig*(i+1), nr_eofs)\n",
    "    if upper <= lower:\n",
    "        break\n",
    "    ims = cplt.mfp(LON, LAT, (eof_re[lower:upper]*landMask).transpose(1,2,0),\n",
    "                   projections=cplt.ccrs.PlateCarree(), extents=(-120, 40, -40, 90), titles=list(range(lower + 1,upper + 1)),\n",
    "                   figsize=(16,7),\n",
    "                   # figsize=(12,6),\n",
    "                   # figsize=(18,6),\n",
    "                   fig_num = 8 + i,\n",
    "                   one_fig_layout=ofl,\n",
    "                   # mx=vmax,\n",
    "                   # colorbar='shared',\n",
    "                   colorbar='individual',\n",
    "                   put_colorbar=False,\n",
    "                   # apply_tight_layout=False\n",
    "                  )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0\n",
    "\n",
    "cplt.mfp(LON, LAT, eof_re[e:e+1].transpose(1,2,0),\n",
    "                   projections=cplt.ccrs.PlateCarree(), extents=None, titles=None,\n",
    "                   # figsize=(18,9),\n",
    "                   figsize=(12,6),\n",
    "                   fig_num = 2,\n",
    "                   one_fig_layout=False,\n",
    "                   # mx=vmax,\n",
    "                   # colorbar='shared',\n",
    "                   colorbar='individual',\n",
    "                   # put_colorbar=False,\n",
    "                   # apply_tight_layout=False\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can determine the lag of each AR model for each PC. This is done by computing the Partial Autocorrelation Function for each index in the PCs.\n",
    "We set a significance level as $\\alpha = 0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 200\n",
    "pacf_lag_re = np.zeros(nr_eofs)\n",
    "acf_lag_re = np.zeros(nr_eofs)\n",
    "thresholdList_re = np.zeros([nr_eofs, lags+1])\n",
    "confList_re = np.zeros(nr_eofs)\n",
    "for i in tqdm(range(nr_eofs)):\n",
    "    p_re, conf_re = pacf(pc_re[:,i],alpha = 0.0001, nlags = lags, method = 'ywm') # the pacf is computed for each time series\n",
    "    conf_set_re = np.asarray([conf_re[j,1]-p_re[j] for j in range(lags+1)]) #The confidence interval is set\n",
    "    pacf_lag_re[i] = np.amax(np.nonzero(np.where(np.abs(p_re)>conf_set_re,1,0))) # masking the pacf with 1 or 0 when inside or outside of confidence interval\n",
    "    thresholdList_re[i,:] = np.abs(p_re) \n",
    "    confList_re[i] = conf_set_re[1]\n",
    "    \n",
    "print(pacf_lag_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the lag estimated for each PC\n",
    "plt.figure()\n",
    "plt.plot(pacf_lag_re, '-o',label = 'pacf')\n",
    "plt.xlabel('PC')\n",
    "plt.ylabel('Max Lag (re)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(5)\n",
    "fig,ax = plt.subplots(figsize=(9,6), num=5)\n",
    "\n",
    "plt.plot(pc_re[:,2])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yule-Walker\n",
    "We use the Yule-Walker method to estimate the AR-model for each PC given the lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_m = int(np.amax(pacf_lag_re)) # Find the maximal lag used and make an [maxLag x Nr_eofs] array - this seems faster than treating Nr_eofs arrays individually\n",
    "rho, sig= np.zeros([nr_eofs,l_m]), np.zeros(nr_eofs)  \n",
    "for i in range(nr_eofs): # for each eof/PC-index we fit an AR(n)-model to the time series\n",
    "    rho[i,:int(pacf_lag_re[i])], sig[i] = yule_walker(pc_re[:,i],order = int(pacf_lag_re[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the noise product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory\n",
    "os.makedirs(noise_product_name, exist_ok=True)\n",
    "\n",
    "# land mask\n",
    "np.save(f'{noise_product_name}/landMask.npy',landMask) \n",
    "\n",
    "# number of autoregreesive terms for each eof\n",
    "np.save(f'{noise_product_name}/Lags.npy', pacf_lag_re)\n",
    "\n",
    "#EOFs and their time-series\n",
    "np.save(f'{noise_product_name}/EOFs.npy',eof_re[:nr_eofs])\n",
    "np.save(f'{noise_product_name}/PCs.npy',pc_re[:,:nr_eofs])\n",
    "\n",
    "#coefficients and sigma for the white noise\n",
    "np.save(f'{noise_product_name}/yw_rho.npy', rho)\n",
    "np.save(f'{noise_product_name}/yw_sigma.npy', sig)\n",
    "\n",
    "logging.log(45, f\"\\n\\nNow save and clear all output of this notebook and save a copy inside '{noise_product_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Noise Model\n",
    "We set up the noise field generator with an initialization and a step function.\n",
    "This is essentially the same noise as on the cluster in the setup_5y_new directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    pc_lag_re = np.zeros([nr_eofs,l_m])\n",
    "    pc_lag_re[:,0] = np.zeros_like(nr_eofs)\n",
    "    pc_series_re = np.zeros(nr_eofs)\n",
    "\n",
    "    grid = np.zeros([2,40,90])\n",
    "    for p in range(nr_eofs):\n",
    "        lag = int(pacf_lag_re[p])\n",
    "        pc_lag_re[p,l_m-1] = np.dot(pc_lag_re[p,:lag],rho[p,:lag])+np.random.normal(0,sig[p]) \n",
    "        pc_lag_re[p,:] = np.roll(pc_lag_re[p,:],1)\n",
    "    pc_series_re = pc_lag_re[:,0]\n",
    "    grid[1] = np.tensordot(pc_series_re,eof_re[:nr_eofs], axes = ([0],[0]))\n",
    "    \n",
    "    return grid, pc_lag_re\n",
    "\n",
    "def step(grid, pc_lag_re):\n",
    "    grid = np.flip(grid,axis = 0)\n",
    "    for p in range(nr_eofs):\n",
    "        lag = int(pacf_lag_re[p])\n",
    "        pc_lag_re[p,l_m-1] = np.dot(pc_lag_re[p,:lag],rho[p,:lag])+np.random.normal(0,sig[p]) \n",
    "        pc_lag_re[p,:] = np.roll(pc_lag_re[p,:],1)\n",
    "    pc_series_re = pc_lag_re[:,0] \n",
    "    grid[1] = np.tensordot(pc_series_re, eof_re[:nr_eofs], axes = ([0],[0]))\n",
    "    return grid, pc_lag_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid, pc = initialize() #initialize the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): # run 10 steps and show the grids\n",
    "    grid, pc = step(grid,pc)\n",
    "    plt.figure()\n",
    "    plt.imshow(grid[0],vmin=-2,vmax = 1)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "The time it takes to simulate 100 and 1000 years of noise at a monthly resolution is estimated. On my PC this took approx. 5 min. to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fortime100(grid,pc):\n",
    "    for i in range(12*100):\n",
    "        step(grid,pc)\n",
    "\n",
    "def fortime1000(grid,pc):\n",
    "    for i in range(12*1000):\n",
    "        step(grid,pc)\n",
    "%timeit fortime100(grid, pc) # time to simulate 100 years of noise\n",
    "%timeit fortime1000(grid, pc) # time to simulate 1000 years of noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "We determine the NAO, AMO and ENSO from the data and the simulation and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = np.empty([12*1000,40,90])\n",
    "grid, pc = initialize() \n",
    "for i in range(12*1000):\n",
    "    grid, pc = step(grid,pc)\n",
    "    grids[i,:,:] = grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAO(grids):\n",
    "    NAO_eof_solver = Eof((grids*landMask))\n",
    "    NAO_eof = NAO_eof_solver.eofs(neofs=2)\n",
    "    NAO_index = NAO_eof_solver.pcs(npcs=1)\n",
    "    return NAO_index, NAO_eof[:,0:14,45:45+25]\n",
    "\n",
    "def ENSO(grids):    \n",
    "    ONI = uniform_filter1d(np.nanmean(grids[:,10:25,20:50],axis = (1,2)),3)#3 mo rolling average\n",
    "    #sol_EN= Eof(grid[:,10:25,20:50])#EOF over slightly bigger area\n",
    "    #c_EN = sol_EN.pcs(pcscaling = 1, npcs = 1)\n",
    "    #EN_eof = sol_EN.eofsAsCorrelation(neofs = 2)\n",
    "    \n",
    "    full_EN= Eof(grids)\n",
    "    EOF = full_EN.eofs(neofs = 1)\n",
    "    #for i in range(20):\n",
    "        #EOF[i] = landmask(EOF[i])\n",
    "    return ONI, EOF[:,10:25,20:50]\n",
    "\n",
    "def AMO(grids):\n",
    "    time = np.shape(grids)[0]\n",
    "    amo = np.zeros(time)\n",
    "    for j in range(time):\n",
    "        glob_mean = np.nanmean(grids[j])\n",
    "        amo[j] = np.nanmean(grids[j,5:15,47:45+20])-glob_mean\n",
    "    return uniform_filter1d(amo,120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAO_index_sim, NAO_sim = NAO(grids)\n",
    "NAO_index_data, NAO_data = NAO(rescaledGrids*landMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(NAO_sim[0])\n",
    "plt.colorbar()\n",
    "plt.title('NAO SIM')\n",
    "plt.figure()\n",
    "plt.imshow(NAO_data[0])\n",
    "plt.colorbar()\n",
    "plt.title('NAO DATA')\n",
    "plt.figure()\n",
    "plt.plot(50+NAO_index_sim[:np.shape(NAO_index_data)[0]],label = 'sim')\n",
    "plt.plot(NAO_index_data,label = 'data')\n",
    "plt.legend()\n",
    "np.std(NAO_index_data),np.std(NAO_index_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSO_index_sim, ENSO_sim = ENSO(grids)\n",
    "ENSO_index_data, ENSO_data = ENSO(rescaledGrids*landMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(ENSO_sim[0])\n",
    "plt.colorbar()\n",
    "plt.title('ENSO SIM')\n",
    "plt.figure()\n",
    "plt.imshow(ENSO_data[0])\n",
    "plt.colorbar()\n",
    "plt.title('ENSO DATA')\n",
    "plt.figure()\n",
    "plt.plot(3+ENSO_index_sim[:np.shape(ENSO_index_data)[0]],label = 'sim')\n",
    "plt.plot(ENSO_index_data,label = 'data')\n",
    "plt.legend()\n",
    "np.std(ENSO_index_data),np.std(ENSO_index_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMO_sim = AMO(grids)\n",
    "AMO_data = AMO(rescaledGrids*landMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(AMO_sim[:np.shape(AMO_data)[0]],label = 'sim')\n",
    "plt.plot(AMO_data,label = 'data')\n",
    "plt.title('AMO')\n",
    "plt.legend()\n",
    "np.std(AMO_data),np.std(AMO_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POINT_SEASON_red(grid,lat, lon):\n",
    "    time = np.shape(grid)[0]\n",
    "    point = grid[:,lat,lon]\n",
    "    season_band = np.zeros([int(np.floor(time/12)),12])\n",
    "    for i in range(int(np.floor(time/12))):\n",
    "        season_band[i,:] = point[i:i+12]\n",
    "    if np.isnan(season_band).all():\n",
    "        return [np.nan,np.nan]\n",
    "    else:\n",
    "        av_season = np.nanmean(season_band,axis = 0)\n",
    "        return [np.abs(max(av_season)-min(av_season)),np.nanstd(grid[:,lat,lon])/np.nanstd(av_season)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledForcing = np.load('HEAVY--rescaledForcing.npy')*landMask[1:]\n",
    "ratios_res = np.zeros([39,90])\n",
    "ratios_std = np.zeros([39,90])\n",
    "ratios_model = np.zeros([39,90])\n",
    "ratios_forcing = np.zeros([39,90])\n",
    "band_res = np.zeros(39)\n",
    "band_std = np.zeros(39)\n",
    "band_model = np.zeros(39)\n",
    "band_forcing = np.zeros(39)\n",
    "\n",
    "for i in range(39):\n",
    "    for j in range(90):\n",
    "        forcing_vals = POINT_SEASON_red(rescaledForcing,i,j)\n",
    "        model_vals = POINT_SEASON_red(grids[:,1:,:],i,j)\n",
    "        #print(model_vals[0])\n",
    "        ratios_res[i,j] = forcing_vals[0]/model_vals[0]\n",
    "        ratios_std[i,j] = model_vals[1]\n",
    "        ratios_model[i,j] = model_vals[0]\n",
    "        ratios_forcing[i,j] = forcing_vals[0]  \n",
    "    band_res[i] = np.nanmean(ratios_res[i,:])\n",
    "    band_std[i] = np.nanmean(ratios_std[i,:])\n",
    "    band_model[i] = np.nanmean(ratios_model[i,:])\n",
    "    band_forcing[i] = np.nanmean(ratios_forcing[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,1,sharex=True, tight_layout = True,gridspec_kw={'height_ratios': [2.5, 1,1]})\n",
    "ax[0].semilogy(np.linspace(17.13402-90,167.62-90,39),np.flip(band_res),label = '$S_{forcing}$/$S_{model}$')\n",
    "ax[0].semilogy(np.linspace(17.13402-90,167.62-90,39),np.flip(band_std),label = r'$\\sigma_{temp}$/$\\sigma_{res. season}$')\n",
    "ax[1].semilogy(np.linspace(17.13402-90,167.62-90,39),np.flip(band_forcing),label = '$S_{forcing}$',color = 'black')\n",
    "ax[2].semilogy(np.linspace(17.13402-90,167.62-90,39),np.flip(band_model),label = '$S_{model}$', color = 'black')\n",
    "ax[2].set_xticks(np.arange(10-90,180-90,20))\n",
    "ax[2].set_yticks(np.arange(0.014,0.022,1))\n",
    "ax[2].set_xlabel('Lattitude at longitude band [$\\degree$]')\n",
    "fig.text(0.04, 0.5, 'Unitless Measure', va='center', rotation='vertical')\n",
    "fig.suptitle('Seasonal strength ($S$) ratio for Forcing/Model and'\n",
    "         '\\n'\n",
    "         'ratio of std. ($\\sigma$) for model temp. anomaly and res. seasonality.')\n",
    "ax[0].legend()\n",
    "ax[1].set_title('$S_{forcing}$')\n",
    "ax[2].set_title('$S_{model}$')\n",
    "#fig.savefig('Composite_season_reduced', dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_find_return(time_series):\n",
    "    freq_plot = plt.psd(time_series,window = mlab.window_none,NFFT = 2**(int(np.floor(len(time_series)/2)-1).bit_length()), noverlap = 2**(int(np.floor(len(time_series)/8)-1).bit_length()))\n",
    "    return [freq_plot[1],freq_plot[0]]\n",
    "def fit_func(x,alpha,beta):\n",
    "    return alpha*(1/(x**beta))\n",
    "def fit_func_lin(x,alpha,beta):\n",
    "    return alpha+x*beta\n",
    "\n",
    "def color(freqs0,freqs1,freq0Name, freq1Name, title):\n",
    "    \n",
    "    \n",
    "    pars0, cov0 = curve_fit(f= fit_func_lin,xdata = np.log10(freqs0[0][1:]),\n",
    "                         ydata = np.log10(freqs0[1][1:]), p0 = [5,1]) \n",
    "    \n",
    "    plt.figure(figsize = (8,4))\n",
    "    plt.loglog(*freqs0,label = freq0Name + ' spectrum')\n",
    "    plt.loglog(*freqs1, label = freq1Name + ' spectrum')\n",
    "    \n",
    "    \n",
    "    \n",
    "    pars1, cov1 = curve_fit(f= fit_func_lin,xdata = np.log10(freqs1[0][1:]),\n",
    "                     ydata = np.log10(freqs1[1][1:]), p0 = [5,1]) \n",
    "    \n",
    "\n",
    "    plt.loglog(10**np.linspace(-3,0,1000), \n",
    "       fit_func(10**np.linspace(-3,0,1000),10**pars0[0], -pars0[1]),color = 'blue',label = freq0Name +r' fit: $\\beta$ = '+str(round(-pars0[1],3)))\n",
    "    \n",
    "    plt.loglog(10**np.linspace(-3,0,1000), \n",
    "           fit_func(10**np.linspace(-3,0,1000),10**pars1[0], -pars1[1]), color = 'yellow',label = freq1Name +r' fit: $\\beta$ = '+str(round(-pars1[1],3)))\n",
    "    plt.title(title + r' spectrum fit to power law $f^{-\\beta}$')\n",
    "    plt.legend()\n",
    "    #print(10**pars0[0],-pars0[1])\n",
    "    #print(10**pars1[0],-pars1[1])\n",
    "    #plt.savefig(title+'_spectrum_fit', dpi = 400)\n",
    "    return pars0,pars1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color(freq_find_return(NAO_index_sim.T[0]),freq_find_return(NAO_index_data.T[0]),'sim ','data ', 'NAO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color(freq_find_return(AMO_sim.T),freq_find_return(AMO_data.T),'sim ','data ', 'NAO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlk",
   "language": "python",
   "name": "mlk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5edd596c062d2ab245267d3f87b34768813c7655af7d910060a729865751fbe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
